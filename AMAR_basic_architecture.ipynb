{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfSnEc_TGU7U"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import numpy as np\n",
        "import json\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from numpy import loadtxt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "\n",
        "def top_scores(predictions,n):\n",
        "  top_n_scores = pd.DataFrame()\n",
        "  for u in list(set(predictions['users'])):\n",
        "    p = predictions.loc[predictions['users'] == u ]\n",
        "    top_n_scores = top_n_scores.append(p.head(n))\n",
        "  return top_n_scores\n",
        "\n",
        "def read_ratings(filename):\n",
        "  user=[]\n",
        "  item=[]\n",
        "  rating=[]\n",
        "  #reading item ids\n",
        "  with open(filename) as csv_file:\n",
        "    csv_reader = csv.reader(csv_file, delimiter='\\t')\n",
        "    #next(csv_reader)\n",
        "    for row in csv_reader:\n",
        "        user.append(int(row[0]))\n",
        "        item.append(int(row[1]))\n",
        "        rating.append(int(row[2]))\n",
        "  return user, item, rating\n",
        "    \n",
        "def read_kale_ratings(filename, folder):\n",
        "    \n",
        "  # load map dataset-matrix\n",
        "  f_map = open(folder+\"_map-dataset-matrix.txt\", \"r\", encoding=\"utf-8\")\n",
        "  dataset_matrix = {}\n",
        "\n",
        "  for line in f_map:\n",
        "      \n",
        "      dataset_value = line.split(\"\\t\")[0].strip()\n",
        "      matrix_value = line.split(\"\\t\")[1].strip()\n",
        "      \n",
        "      dataset_matrix[dataset_value] = matrix_value\n",
        "\n",
        "  f_map.close()\n",
        "\n",
        "\n",
        "  user=[]\n",
        "  item=[]\n",
        "  rating=[]\n",
        "\n",
        "  #reading item ids\n",
        "  with open(filename) as csv_file:\n",
        "    csv_reader = csv.reader(csv_file, delimiter='\\t')\n",
        "    #next(csv_reader)\n",
        "    for row in csv_reader:\n",
        "        if row[0] in dataset_matrix and row[1] in dataset_matrix:\n",
        "            user.append(int(row[0]))\n",
        "            item.append(int(row[1]))\n",
        "            rating.append(int(row[2]))\n",
        "  return user, item, rating\n",
        "\n",
        "def read_kale_embeddings(folder):\n",
        "\n",
        "  # load map dataset-matrix\n",
        "  f_map = open(folder+\"_map-dataset-matrix.txt\", \"r\", encoding=\"utf-8\")\n",
        "  #f_map = open(folder+\"map.txt\", \"r\", encoding=\"utf-8\")\n",
        "  dataset_matrix = {}\n",
        "\n",
        "  for line in f_map:\n",
        "      \n",
        "      dataset_value = line.split(\"\\t\")[0].strip()\n",
        "      matrix_value = line.split(\"\\t\")[1].strip()\n",
        "      \n",
        "      dataset_matrix[dataset_value] = matrix_value\n",
        "\n",
        "  f_map.close()\n",
        "\n",
        "  # load items embeddings\n",
        "  filein = folder+\"MatrixE.best\"\n",
        "\n",
        "  # open file\n",
        "  f_in = open(filein, mode=\"r\", encoding=\"utf-8\")\n",
        "\n",
        "  # read header\n",
        "  header = f_in.readline().split(\";\")\n",
        "  rows = int(header[0].split(\":\")[1].strip())\n",
        "  columns = int(header[1].split(\":\")[1].strip())\n",
        "\n",
        "  # matrix inizialization\n",
        "  embeddings = np.zeros((rows, columns))\n",
        "  row = 0\n",
        "  col = 0\n",
        "\n",
        "  # file reading and embeddings population\n",
        "  for vector in f_in:\n",
        "      emb = vector.split(\"\\t\")\n",
        "      col = 0\n",
        "      for value in emb:\n",
        "          embeddings[row][col] = float(value)\n",
        "          col += 1\n",
        "      row += 1\n",
        "      \n",
        "  f_in.close()\n",
        "\n",
        "  return embeddings, dataset_matrix\n",
        "  \n",
        "\n",
        "\n",
        "def matching_kale_emb_id(user, item, rating, embeddings, dataset_matrix):\n",
        "  y = np.array(rating)\n",
        "  dim_embeddings = len(embeddings[0])\n",
        "  dim_X_cols = 2\n",
        "  dim_X_rows = len(user)\n",
        "  X = np.empty(shape=(dim_X_rows,dim_X_cols,dim_embeddings))\n",
        "\n",
        "      \n",
        "  #matching between ids and embeddings\n",
        "  i=0\n",
        "  c=0\n",
        "  while i < dim_X_rows:\n",
        "\n",
        "      # get user and item id\n",
        "      user_id = user[i]\n",
        "      item_id = item[i]\n",
        "          \n",
        "      if str(user_id) in dataset_matrix and str(item_id) in dataset_matrix:\n",
        "          \n",
        "          # get indeces of the matrix related to user and item from the map\n",
        "          ind_user = int(dataset_matrix[str(user_id)])\n",
        "          ind_item = int(dataset_matrix[str(item_id)])\n",
        "              \n",
        "          # get embeddings of user and item\n",
        "          X[c][0] = embeddings[ind_user]\n",
        "          X[c][1] = embeddings[ind_item]\n",
        "          \n",
        "          c += 1\n",
        "          \n",
        "      i=i+1\n",
        "      \n",
        "      \n",
        "  return X, y, dim_embeddings\n",
        "\n",
        "def isolate_kale_user_item_emb(users, items, graph_embeddings, dataset_matrix):\n",
        "\n",
        "  embs = []\n",
        "  i=0\n",
        "  user_map = {}\n",
        "  item_map = {}\n",
        "\n",
        "  for usr in users:\n",
        "    ind_u = int(dataset_matrix[str(usr)])\n",
        "    embs.append(graph_embeddings[ind_u])\n",
        "    user_map[usr] = i\n",
        "    i+=1\n",
        "\n",
        "  for itm in items:\n",
        "    ind_i = int(dataset_matrix[str(itm)])\n",
        "    embs.append(graph_embeddings[ind_i])\n",
        "    item_map[itm] = i\n",
        "    i+=1\n",
        "\n",
        "\n",
        "  return embs, user_map, item_map\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from numpy import loadtxt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "#from models.DataGenerator import DataGenerator as dg\n",
        "\n",
        "# define the keras model\n",
        "def run_model(X, y, dim_embeddings, epochs, batch_size):\n",
        "  model = keras.Sequential()\n",
        "\n",
        "  input_users = keras.layers.Input(shape=(dim_embeddings,))\n",
        "\n",
        "  x1 = keras.layers.Dense(512, activation=tf.nn.relu)(input_users)\n",
        "  x1_2 = keras.layers.Dense(256, activation=tf.nn.relu)(x1)\n",
        "  x1_3 = keras.layers.Dense(128, activation=tf.nn.relu)(x1_2)\n",
        "\n",
        "  input_items = keras.layers.Input(shape=(dim_embeddings,))\n",
        "\n",
        "  x2 = keras.layers.Dense(512, activation=tf.nn.relu)(input_items)\n",
        "  x2_2 = keras.layers.Dense(256, activation=tf.nn.relu)(x2)\n",
        "  x2_3 = keras.layers.Dense(128, activation=tf.nn.relu)(x2_2)\n",
        "\n",
        "  concatenated = keras.layers.Concatenate()([x1_3, x2_3])\n",
        "\n",
        "  d1 = keras.layers.Dense(64, activation=tf.nn.relu)(concatenated)\n",
        "  d2 = keras.layers.Dense(64, activation=tf.nn.relu)(d1)\n",
        "  out = keras.layers.Dense(1, activation=tf.nn.sigmoid)(d2)\n",
        "\n",
        "  model = keras.models.Model(inputs=[input_users,input_items],outputs=out)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9), metrics=['accuracy'])\n",
        "  model.fit([X[:,0],X[:,1]], y, epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os.path as path\n",
        "import tensorflow as tf\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "print(\"start\")\n",
        "\n",
        "# folder with embeddings folders. \n",
        "# each folder must have a file called \"MatrixE.best\" containing the embeddings of the entities \n",
        "# and a file called \"_map-dataset-matrix.txt\" which maps KALE id with dataset ID and viceversa\n",
        "maindir = \"/content/gdrive/MyDrive/amar/embeddings/\"\n",
        "folders = os.listdir(maindir)\n",
        "print(folders)\n",
        "\n",
        "# for each folder (configuration) of a given dimension\n",
        "for folder in folders:\n",
        "       \n",
        "    print(\"Starting \" + folder)\n",
        "    \n",
        "    # set input files and output files\n",
        "    source = maindir + folder + '/'\n",
        "    dest = '/content/gdrive/MyDrive/amar/results/' + folder +'/'\n",
        "    prediction_dest = '/content/gdrive/MyDrive/amar/predictions/' + folder\n",
        "\n",
        "    dest = '/content/gdrive/MyDrive/amar/results/' + folder +'/'\n",
        "    prediction_dest = '/content/gdrive/MyDrive/amar/predictions/ + folder\n",
        "    \n",
        "    # create folders if needed\n",
        "    if not os.path.isdir(dest):\n",
        "        os.mkdir(dest)\n",
        "        print(\"\\tCreated dir \" + dest)\n",
        "        \n",
        "    if not os.path.isdir(prediction_dest):\n",
        "        os.mkdir(prediction_dest)\n",
        "        os.mkdir(prediction_dest+'/top_5/')\n",
        "        os.mkdir(prediction_dest+'/top_10/')\n",
        "        print(\"\\tCreated dir \" + prediction_dest)\n",
        "        \n",
        "\n",
        "    # load kale embeddings for training and the map dataset -> matrix\n",
        "    ent_embeddings, dataset_matrix = read_kale_embeddings(source)\n",
        "    \n",
        "    # read user-item/user-item-prop train set\n",
        "    user, item, rating = read_ratings('/content/gdrive/MyDrive/amar/datasets/movielens1m/useritem_train.tsv')\n",
        "    \n",
        "    # match KALE ids with dataset ids\n",
        "    X, y, dim_embeddings = matching_kale_emb_id(user, item, rating, ent_embeddings, dataset_matrix)\n",
        "    \n",
        "    print(\"\\tEmbedding dimension: \", dim_embeddings)    \n",
        "    \n",
        "    # run model\n",
        "    batch = 512\n",
        "    epo = 25\n",
        "    model = run_model(X, y, dim_embeddings, epochs=epo, batch_size=batch)\n",
        "        \n",
        "    # creates a HDF5 file 'model.h5'\n",
        "    model.save(dest + 'model.h5')\n",
        "    \n",
        "    # load test ratings to produce predictions\n",
        "    user, item, rating = read_ratings('/content/gdrive/MyDrive/amar/datasets/movielens1m/useritem_test.tsv')\n",
        "    \n",
        "    # select kale embeddings for predictions\n",
        "    X, y, dim_embeddings = matching_kale_emb_id(user, item, rating, ent_embeddings, dataset_matrix)\n",
        "    \n",
        "    # predict   \n",
        "    print(\"\\tPredicting...\")\n",
        "    score = model.predict([X[:,0],X[:,1]])\n",
        "    \n",
        "    # write predictions\n",
        "    print(\"\\tComputing predictions...\")\n",
        "    score = score.reshape(1, -1)[0,:]\n",
        "    predictions = pd.DataFrame()\n",
        "    predictions['users'] = np.array(user)\n",
        "    predictions['items'] = np.array(item)\n",
        "    predictions['scores'] = score\n",
        "    \n",
        "    predictions = predictions.sort_values(by=['users', 'scores'],ascending=[True, False])\n",
        "    \n",
        "    top_5_scores = top_scores(predictions,5)\n",
        "    top_5_scores.to_csv(prediction_dest + '/top_5/predictions.tsv',sep='\\t',header=False,index=False)\n",
        "    print(\"\\tSuccessfully writing top 5 scores\")\n",
        "    \n",
        "    top_10_scores = top_scores(predictions,10)\n",
        "    top_10_scores.to_csv(prediction_dest + '/top_10/predictions.tsv',sep='\\t',header=False,index=False)\n",
        "    print(\"\\tSuccessfully writing top 10 scores\")\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QdkATBZGm1t"
      },
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "AMAR_basic_architecture.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}